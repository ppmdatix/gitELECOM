{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.core import Reshape, Dense, Dropout, Flatten\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import Convolution2D, UpSampling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.datasets import mnist\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras import initializers\n",
    "import tensorflow as tf\n",
    "\n",
    "from ploting import plotGeneratedImages, plotLoss\n",
    "from saving import saveModels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "#warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.set_image_dim_ordering('th')\n",
    "\n",
    "# Deterministic output.\n",
    "# Tired of seeing the same results every time? Remove the line below.\n",
    "np.random.seed(1000)\n",
    "\n",
    "# The results are a little better when the dimensionality of the random vector is only 10.\n",
    "# The dimensionality has been left at 100 for consistency with other GAN implementations.\n",
    "randomDim = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = (X_train.astype(np.float32) - 127.5)/127.5\n",
    "X_train = X_train.reshape(60000, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/peseux/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# Optimizer\n",
    "adam = Adam(lr=0.0002, beta_1=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Sequential()\n",
    "generator.add(Dense(256, input_dim=randomDim, kernel_initializer=initializers.RandomNormal(stddev=0.02)))\n",
    "generator.add(LeakyReLU(0.2))\n",
    "generator.add(Dense(512))\n",
    "generator.add(LeakyReLU(0.2))\n",
    "generator.add(Dense(1024))\n",
    "generator.add(LeakyReLU(0.2))\n",
    "generator.add(Dense(784, activation='tanh'))\n",
    "generator.compile(loss=\"binary_crossentropy\", optimizer=adam)\n",
    "# generator.compile(loss='binary_crossentropy', optimizer=adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/peseux/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "discriminator = Sequential()\n",
    "discriminator.add(Dense(1024, input_dim=784, kernel_initializer=initializers.RandomNormal(stddev=0.02)))\n",
    "discriminator.add(LeakyReLU(0.2))\n",
    "discriminator.add(Dropout(0.3))\n",
    "discriminator.add(Dense(512))\n",
    "discriminator.add(LeakyReLU(0.2))\n",
    "discriminator.add(Dropout(0.3))\n",
    "discriminator.add(Dense(256))\n",
    "discriminator.add(LeakyReLU(0.2))\n",
    "discriminator.add(Dropout(0.3))\n",
    "discriminator.add(Dense(1, activation='sigmoid'))\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=adam)\n",
    "\n",
    "# Combined network\n",
    "discriminator.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Loss\n",
    "def customLoss(layer_weights, lamda, alpha=1, y_func=\"pow10\"):\n",
    "    def lossFunction(y_true,y_pred):\n",
    "        y = K.log( - K.abs(y_pred - y_true))\n",
    "        x = lamda * (K.mean(K.abs(( - layer_weights + 1)/2)))#/2)) + 0.5*K.std(layer_weights)) / 2\n",
    "        if y_func == \"carre\":\n",
    "            fy = y*y\n",
    "        elif y_func == \"pow10\":\n",
    "            fy = y**10\n",
    "        else:\n",
    "            fy = y\n",
    "        #loss = x * K.log( - K.abs(y_pred - y_true)) + (1 - x)*(K.log( - K.abs(y_pred - y_true))*K.log( - K.abs(y_pred - y_true)))\n",
    "        loss = x + y\n",
    "        return  loss\n",
    "\n",
    "    return lossFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateGeneratedImagesMalveillance(generator, randomDim, examples=10, print_mode=False):\n",
    "    noise = np.random.normal(0, 1, size=[examples, randomDim])\n",
    "    generatedImages = generator.predict(noise)\n",
    "    result = [np.mean(np.abs(- x +np.ones(784))/2) for x in generatedImages]\n",
    "    if print_mode:\n",
    "        plt.plot(result)\n",
    "        plt.title\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    return np.mean(result)\n",
    "\n",
    "def evaluateGeneratedImagesFooling(discriminator, generator, randomDim, examples=10, print_mode=False):\n",
    "    noise = np.random.normal(0, 1, size=[examples, randomDim])\n",
    "    generatedImages = generator.predict(noise)\n",
    "    y_pred = discriminator.predict(generatedImages)\n",
    "    result = [np.mean(- np.log(np.abs(y_pred - np.ones(1))))]\n",
    "   \n",
    "    if print_mode:\n",
    "        plt.plot(result)\n",
    "        plt.title\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    return np.mean(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ganInput = Input(shape=(randomDim,))\n",
    "x = generator(ganInput)\n",
    "ganOutput = discriminator(x)\n",
    "gan = Model(inputs=ganInput, outputs=ganOutput)\n",
    "lamda = .04\n",
    "gan.compile(loss=customLoss(x, lamda), optimizer =adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLosses = []\n",
    "gLosses = []\n",
    "mLosses = []\n",
    "fLosses = []\n",
    "def train(epochs=1, batchSize=128, dLossLimit=0.1):\n",
    "    global dLosses, gLosses, mLosses, fLosses\n",
    "    \n",
    "    toBeTrusted = True\n",
    "    batchCount = X_train.shape[0] / batchSize\n",
    "    print('Epochs:', epochs)\n",
    "    print('Batch size:', batchSize)\n",
    "    print('Batches per epoch:', batchCount)\n",
    "\n",
    "    for e in range(1, epochs+1):\n",
    "        print('-'*15, 'Epoch %d' % e, '-'*15)\n",
    "        for _ in tqdm(range(int(batchCount))):\n",
    "            # Get a random set of input noise and images\n",
    "            noise = np.random.normal(0, 1, size=[batchSize, randomDim])\n",
    "            imageBatch = X_train[np.random.randint(0, X_train.shape[0], size=batchSize)]\n",
    "\n",
    "            # Generate fake MNIST images\n",
    "            generatedImages = generator.predict(noise)\n",
    "            # print np.shape(imageBatch), np.shape(generatedImages)\n",
    "            X = np.concatenate([imageBatch, generatedImages])\n",
    "\n",
    "            # Labels for generated and real data\n",
    "            yDis = np.zeros(2*batchSize)\n",
    "            # One-sided label smoothing\n",
    "            yDis[:batchSize] = 0.9\n",
    "\n",
    "            # Train discriminator\n",
    "            discriminator.trainable = True\n",
    "            dloss = discriminator.train_on_batch(X, yDis)\n",
    "\n",
    "            # Train generator\n",
    "            noise = np.random.normal(0, 1, size=[batchSize, randomDim])\n",
    "            yGen = np.ones(batchSize)\n",
    "            discriminator.trainable = False\n",
    "            gloss = gan.train_on_batch(noise, yGen)\n",
    "\n",
    "        if dloss < dLossLimit:\n",
    "            toBeTrusted = False\n",
    "            break\n",
    "        # Store loss of most recent batch from this epoch\n",
    "        dLosses.append(dloss)\n",
    "        gLosses.append(gloss)\n",
    "        \n",
    "        # Specific Loss M\n",
    "        mLosses.append(evaluateGeneratedImagesMalveillance(generator=generator, randomDim=randomDim, examples=10))\n",
    "        fLosses.append(evaluateGeneratedImagesFooling(discriminator=discriminator, generator=generator, randomDim=randomDim, examples=10))\n",
    "\n",
    "\n",
    "        if e == 1 or e % 6 == 0:\n",
    "            saveModels(e, generator=generator, discriminator=discriminator)\n",
    "            plotGeneratedImages(e, generator=generator, randomDim=randomDim)\n",
    "    # Plot losses from every epoch\n",
    "    if toBeTrusted:\n",
    "        plotLoss(e, dLosses=dLosses, gLosses=gLosses)\n",
    "        return True\n",
    "    else: \n",
    "        print(\"=========ERROR=========\")\n",
    "        print(\"The Disc-Loss goesssssssssss friend\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/468 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 135\n",
      "Batch size: 128\n",
      "Batches per epoch: 468.75\n",
      "--------------- Epoch 1 ---------------\n",
      "WARNING:tensorflow:From /home/peseux/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:42<00:00, 11.00it/s]\n",
      "  0%|          | 2/468 [00:00<00:29, 15.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 2 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:30<00:00, 15.15it/s]\n",
      "  0%|          | 2/468 [00:00<00:32, 14.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 3 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:32<00:00, 13.62it/s]\n",
      "  0%|          | 2/468 [00:00<00:34, 13.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 4 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:35<00:00, 13.05it/s]\n",
      "  0%|          | 2/468 [00:00<00:29, 15.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 5 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:34<00:00, 13.49it/s]\n",
      "  0%|          | 2/468 [00:00<00:33, 13.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 6 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:34<00:00, 14.48it/s]\n",
      "  0%|          | 2/468 [00:00<00:45, 10.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 7 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:34<00:00, 13.41it/s]\n",
      "  0%|          | 2/468 [00:00<00:30, 15.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 8 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:33<00:00, 15.01it/s]\n",
      "  0%|          | 2/468 [00:00<00:30, 15.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 9 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:34<00:00, 13.63it/s]\n",
      "  0%|          | 2/468 [00:00<00:31, 14.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 10 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:35<00:00, 14.33it/s]\n",
      "  0%|          | 2/468 [00:00<00:34, 13.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 11 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:34<00:00, 13.59it/s]\n",
      "  0%|          | 2/468 [00:00<00:31, 14.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 12 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:31<00:00, 14.36it/s]\n",
      "  0%|          | 2/468 [00:00<00:30, 15.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 13 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:34<00:00, 15.42it/s]\n",
      "  0%|          | 2/468 [00:00<00:29, 15.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 14 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:32<00:00, 14.97it/s]\n",
      "  0%|          | 2/468 [00:00<00:29, 15.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 15 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:32<00:00, 14.28it/s]\n",
      "  0%|          | 2/468 [00:00<00:34, 13.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 16 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:32<00:00, 15.24it/s]\n",
      "  0%|          | 2/468 [00:00<00:29, 15.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 17 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:33<00:00, 12.88it/s]\n",
      "  0%|          | 2/468 [00:00<00:31, 14.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 18 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:37<00:00, 13.61it/s]\n",
      "  0%|          | 2/468 [00:00<00:37, 12.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 19 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:33<00:00, 13.98it/s]\n",
      "  0%|          | 2/468 [00:00<00:34, 13.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 20 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:35<00:00, 13.65it/s]\n",
      "  0%|          | 2/468 [00:00<00:32, 14.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 21 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:34<00:00, 14.63it/s]\n",
      "  0%|          | 2/468 [00:00<00:30, 15.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 22 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:36<00:00, 14.07it/s]\n",
      "  0%|          | 2/468 [00:00<00:29, 15.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 23 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:36<00:00, 12.33it/s]\n",
      "  0%|          | 2/468 [00:00<00:37, 12.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 24 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:36<00:00, 12.76it/s]\n",
      "  0%|          | 2/468 [00:00<00:42, 11.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 25 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:33<00:00, 13.44it/s]\n",
      "  0%|          | 2/468 [00:00<00:32, 14.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 26 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:33<00:00, 13.17it/s]\n",
      "  0%|          | 2/468 [00:00<00:33, 14.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 27 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:34<00:00, 13.19it/s]\n",
      "  0%|          | 2/468 [00:00<00:29, 15.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 28 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:34<00:00, 14.12it/s]\n",
      "  0%|          | 2/468 [00:00<00:35, 13.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 29 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:34<00:00, 13.22it/s]\n",
      "  0%|          | 1/468 [00:00<00:57,  8.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 30 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:35<00:00, 13.45it/s]\n",
      "  0%|          | 2/468 [00:00<00:29, 15.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 31 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:33<00:00, 14.78it/s]\n",
      "  0%|          | 2/468 [00:00<00:32, 14.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 32 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:33<00:00, 13.87it/s]\n",
      "  0%|          | 2/468 [00:00<00:32, 14.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 33 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:34<00:00, 12.86it/s]\n",
      "  0%|          | 2/468 [00:00<00:35, 13.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 34 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:36<00:00, 12.99it/s]\n",
      "  0%|          | 2/468 [00:00<00:31, 14.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 35 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:32<00:00, 13.61it/s]\n",
      "  0%|          | 2/468 [00:00<00:34, 13.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 36 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:32<00:00, 13.53it/s]\n",
      "  0%|          | 2/468 [00:00<00:29, 15.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 37 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 146/468 [00:10<00:32,  9.93it/s]"
     ]
    }
   ],
   "source": [
    "epochs = 135\n",
    "batchSize = 128\n",
    "\n",
    "train(epochs=epochs, batchSize=batchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mLosses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "\n",
    "# load json and create model\n",
    "json_file = open('models/gan10epochs.json', 'r')\n",
    "loaded_disc = json_file.read()\n",
    "json_file.close()\n",
    "loaded_disc = model_from_json(loaded_disc)\n",
    "# load weights into new model\n",
    "loaded_disc.load_weights(\"models/modelGAN10epochs.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# evaluate loaded model on test data\n",
    "# Optimizer\n",
    "oldadam = Adam(lr=0.0002, beta_1=0.5)\n",
    "\n",
    "loaded_disc.compile(loss='binary_crossentropy', optimizer=oldadam, metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(loaded_disc.predict(generatedImages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = np.random.normal(0, 1, size=[10, 100])\n",
    "generatedImages = generator.predict(noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluateGeneratedImages(generator, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
